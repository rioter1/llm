Notes on GPT  

GPT2 124M model  
state_dcit= Raw tensors  
token embedding = [50257,768] each token is a 768 dim embedding  
position embedding = [1024,768] max sequence length is 1024, 1024 positions need attending from the past  

